{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanakt-hub/Test/blob/main/SCDV_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVFK7il_jPvZ"
      },
      "outputs": [],
      "source": [
        "#ライブドアコーパス(https://www.rondhuit.com/download.html#ldcc)の展開\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!tar zxf ldcc-20140209.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yKt4d4pVl_Le"
      },
      "outputs": [],
      "source": [
        "#サンプル文書の追加\n",
        "import urllib.request\n",
        "SampleDoc = \"https://raw.githubusercontent.com/tanakt-hub/Test/main/data/SampleDoc.txt\"\n",
        "\n",
        "res = urllib.request.urlopen(SampleDoc)\n",
        "res = res.read().decode(\"utf-8\")\n",
        "SplitDoc = res.replace('\\n', '').split('。')\n",
        "\n",
        "tmpDoc = []\n",
        "tmp =\"\"\n",
        "for i in SplitDoc:\n",
        "  if len(i) < 200:\n",
        "    tmp = tmp + i + \"。\"\n",
        "    if len(tmp) >= 200:\n",
        "      tmpDoc.append(tmp)\n",
        "      tmp =\"\"\n",
        "  if len(i) >= 200:\n",
        "    tmpDoc.append(i + \"。\")\n",
        "    tmp =\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MOUWHj44qtZD"
      },
      "outputs": [],
      "source": [
        "#!rm -r /content/text/sampledoc\n",
        "!mkdir /content/text/sampledoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vt1KKDO3oh0w"
      },
      "outputs": [],
      "source": [
        "for c, i in enumerate(tmpDoc):\n",
        "  f = open('/content/text/sampledoc/' + str(c) + '.txt', 'w')\n",
        "  f.write(i)\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8abodyorVPW"
      },
      "outputs": [],
      "source": [
        "#文書をDFに読み込み\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "#preprocessing\n",
        "dirlist = [\"dokujo-tsushin\",\"it-life-hack\",\"kaden-channel\",\"livedoor-homme\", \"movie-enter\",\"peachy\",\"smax\",\"sports-watch\",\"topic-news\", \"sampledoc\"]\n",
        "#dirlist = [\"kaden-channel\"]\n",
        "df = pd.DataFrame(columns=[\"class\",\"news\"])\n",
        "for i in tqdm(dirlist):\n",
        "    path = \"/content/text/\" + i + \"/*.txt\"\n",
        "    files = glob.glob(path)\n",
        "    files.pop()\n",
        "    for j in tqdm(files):\n",
        "        f = open(j)\n",
        "        data = f.read() \n",
        "        f.close()\n",
        "        if i == \"sampledoc\":\n",
        "          t = pd.Series([i,data],index = df.columns)\n",
        "          df  = df.append(t,ignore_index=True)          \n",
        "        if i != \"sampledoc\":\n",
        "          t = pd.Series([i,\"\".join(data.split(\"\\n\")[3:])],index = df.columns)\n",
        "          df  = df.append(t,ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v1t_hjYKrrJ7"
      },
      "outputs": [],
      "source": [
        "#pd.set_option('display.max_rows', None)\n",
        "#print(df[7000:8444])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63SYwTwHr-lE"
      },
      "outputs": [],
      "source": [
        "#MeCabの用意\n",
        "!pip install mecab-python3 unidic-lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G51ECfuRr1vk"
      },
      "outputs": [],
      "source": [
        "## create word2vec\n",
        "import logging\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import MeCab\n",
        "import time\n",
        "from sklearn.preprocessing import normalize\n",
        "import sys\n",
        "import re\n",
        "\n",
        "start = time.time()\n",
        "tokenizer =  MeCab.Tagger(\"-Owakati\")  \n",
        "sentences = []\n",
        "print (\"Parsing sentences from training set...\")\n",
        "\n",
        "# Loop over each news article.\n",
        "for review in tqdm(df[\"news\"]):\n",
        "    try:\n",
        "        # Split a review into parsed sentences.\n",
        "        result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
        "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
        "        h = result.split(\" \")\n",
        "        h = list(filter((\"\").__ne__, h))\n",
        "        sentences.append(h)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "    level=logging.INFO)\n",
        "\n",
        "num_features = 200     # Word vector dimensionality\n",
        "min_word_count = 20   # Minimum word count\n",
        "num_workers = 40       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "print (\"Training Word2Vec model...\")\n",
        "# Train Word2Vec model.\n",
        "model = Word2Vec(sentences, workers=num_workers, hs = 0, sg = 1, negative = 10, iter = 25,\\\n",
        "            size=num_features, min_count = min_word_count, \\\n",
        "            window = context, sample = downsampling, seed=1)\n",
        "\n",
        "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
        "model.init_sims(replace=True)\n",
        "# Save Word2Vec model.\n",
        "print (\"Saving Word2Vec model...\")\n",
        "model.save(\"/content/\"+model_name)\n",
        "endmodeltime = time.time()\n",
        "\n",
        "print (\"time : \", endmodeltime-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SN68s7hovjen"
      },
      "outputs": [],
      "source": [
        "## create gwbowv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
        "import pickle\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def cluster_GMM(num_clusters, word_vectors):\n",
        "    # Initalize a GMM object and use it for clustering.\n",
        "    clf =  GaussianMixture(n_components=num_clusters,\n",
        "                    covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
        "    # Get cluster assignments.\n",
        "    clf.fit(word_vectors)\n",
        "    idx = clf.predict(word_vectors)\n",
        "    print (\"Clustering Done...\", time.time()-start, \"seconds\")\n",
        "    # Get probabilities of cluster assignments.\n",
        "    idx_proba = clf.predict_proba(word_vectors)\n",
        "    # Dump cluster assignments and probability of cluster assignments. \n",
        "    pickle.dump(idx, open('/content/gmm_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
        "    print (\"Cluster Assignments Saved...\")\n",
        "\n",
        "    pickle.dump(idx_proba,open( '/content/gmm_prob_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
        "    print (\"Probabilities of Cluster Assignments Saved...\")\n",
        "    return (idx, idx_proba)\n",
        "\n",
        "def read_GMM(idx_name, idx_proba_name):\n",
        "    # Loads cluster assignments and probability of cluster assignments. \n",
        "    idx = pickle.load(open('/content/gmm_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
        "    idx_proba = pickle.load(open( '/content/gmm_prob_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
        "    print (\"Cluster Model Loaded...\")\n",
        "    return (idx, idx_proba)\n",
        "\n",
        "def get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict):\n",
        "    # This function computes probability word-cluster vectors\n",
        "    prob_wordvecs = {}\n",
        "    for word in word_centroid_map:\n",
        "        prob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
        "        for index in range(0, num_clusters):\n",
        "            try:\n",
        "                prob_wordvecs[word][index*num_features:(index+1)*num_features] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    # prob_wordvecs_idf_len2alldata = {}\n",
        "    # i = 0\n",
        "    # for word in featurenames:\n",
        "    #     i += 1\n",
        "    #     if word in word_centroid_map:    \n",
        "    #         prob_wordvecs_idf_len2alldata[word] = {}\n",
        "    #         for index in range(0, num_clusters):\n",
        "    #                 prob_wordvecs_idf_len2alldata[word][index] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word] \n",
        "\n",
        "\n",
        "\n",
        "    # for word in prob_wordvecs_idf_len2alldata.keys():\n",
        "    #     prob_wordvecs[word] = prob_wordvecs_idf_len2alldata[word][0]\n",
        "    #     for index in prob_wordvecs_idf_len2alldata[word].keys():\n",
        "    #         if index==0:\n",
        "    #             continue\n",
        "    #         prob_wordvecs[word] = np.concatenate((prob_wordvecs[word], prob_wordvecs_idf_len2alldata[word][index]))\n",
        "    return prob_wordvecs\n",
        "\n",
        "def create_cluster_vector_and_gwbowv(prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
        "    # This function computes SDV feature vectors.\n",
        "    bag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
        "    global min_no\n",
        "    global max_no\n",
        "\n",
        "    for word in wordlist:\n",
        "        try:\n",
        "            temp = word_centroid_map[word]\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        bag_of_centroids += prob_wordvecs[word]\n",
        "\n",
        "    norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
        "    if(norm!=0):\n",
        "        bag_of_centroids /= norm\n",
        "\n",
        "    # To make feature vector sparse, make note of minimum and maximum values.\n",
        "    if train:\n",
        "        min_no += min(bag_of_centroids)\n",
        "        max_no += max(bag_of_centroids)\n",
        "\n",
        "    return bag_of_centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkZKJgs2voKW"
      },
      "outputs": [],
      "source": [
        "num_features = 200     # Word vector dimensionality\n",
        "min_word_count = 20   # Minimum word count\n",
        "num_workers = 40       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
        "# Load the trained Word2Vec model.\n",
        "model = Word2Vec.load(\"/content/\"+model_name)\n",
        "# Get wordvectors for all words in vocabulary.\n",
        "word_vectors = model.wv.syn0\n",
        "\n",
        "# Load train data.\n",
        "train,test = train_test_split(df,test_size=0.3,random_state=40)\n",
        "all = df\n",
        "\n",
        "# Set number of clusters.\n",
        "num_clusters = 60\n",
        "idx, idx_proba = cluster_GMM(num_clusters, word_vectors)\n",
        "\n",
        "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
        "# a cluster number\n",
        "word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
        "# Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to\n",
        "# list of probabilities of cluster assignments.\n",
        "word_centroid_prob_map = dict(zip( model.wv.index2word, idx_proba ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UTshsWJvq2n"
      },
      "outputs": [],
      "source": [
        "# Computing tf-idf values.\n",
        "traindata = []\n",
        "for review in all[\"news\"]:\n",
        "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
        "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
        "    h = result.split(\" \")\n",
        "    h = filter((\"\").__ne__, h)\n",
        "    traindata.append(\" \".join(h))\n",
        "\n",
        "tfv = TfidfVectorizer(dtype=np.float32)\n",
        "tfidfmatrix_traindata = tfv.fit_transform(traindata)\n",
        "featurenames = tfv.get_feature_names()\n",
        "idf = tfv._tfidf.idf_\n",
        "\n",
        "# Creating a dictionary with word mapped to its idf value \n",
        "print (\"Creating word-idf dictionary for Training set...\")\n",
        "\n",
        "word_idf_dict = {}\n",
        "for pair in zip(featurenames, idf):\n",
        "    word_idf_dict[pair[0]] = pair[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3GlClATvtDk"
      },
      "outputs": [],
      "source": [
        "# Pre-computing probability word-cluster vectors.\n",
        "prob_wordvecs = get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict)\n",
        "\n",
        "## 該当する関数を再掲\n",
        "def get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict):\n",
        "    # This function computes probability word-cluster vectors\n",
        "    prob_wordvecs = {}\n",
        "    for word in word_centroid_map:\n",
        "        prob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
        "        for index in range(0, num_clusters):\n",
        "            try:\n",
        "                prob_wordvecs[word][index*num_features:(index+1)*num_features] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
        "            except:\n",
        "                continue\n",
        "        return prob_wordvecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6ZrVGIavvec"
      },
      "outputs": [],
      "source": [
        "# gwbowv is a matrix which contains normalised document vectors.\n",
        "gwbowv = np.zeros( (train[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
        "\n",
        "counter = 0\n",
        "\n",
        "min_no = 0\n",
        "max_no = 0\n",
        "for review in train[\"news\"]:\n",
        "    # Get the wordlist in each news article.\n",
        "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
        "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
        "    h = result.split(\" \")\n",
        "    h = filter((\"\").__ne__, h)\n",
        "    words = h\n",
        "    gwbowv[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters, train=True)\n",
        "    counter+=1\n",
        "    if counter % 1000 == 0:\n",
        "        print (\"Train News Covered : \",counter)\n",
        "\n",
        "gwbowv_name = \"SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
        "\n",
        "gwbowv_test = np.zeros( (test[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for review in test[\"news\"]:\n",
        "    # Get the wordlist in each news article.\n",
        "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
        "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
        "    h = result.split(\" \")\n",
        "    h = filter((\"\").__ne__, h)\n",
        "    words = h\n",
        "    gwbowv_test[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters)\n",
        "    counter+=1\n",
        "    if counter % 1000 == 0:\n",
        "        print (\"Test News Covered : \",counter)\n",
        "\n",
        "test_gwbowv_name = \"TEST_SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
        "\n",
        "print (\"Making sparse...\")\n",
        "# Set the threshold percentage for making it sparse. \n",
        "percentage = 0.04\n",
        "min_no = min_no*1.0/len(train[\"news\"])\n",
        "max_no = max_no*1.0/len(train[\"news\"])\n",
        "print (\"Average min: \", min_no)\n",
        "print (\"Average max: \", max_no)\n",
        "thres = (abs(max_no) + abs(min_no))/2\n",
        "thres = thres*percentage\n",
        "\n",
        "# Make values of matrices which are less than threshold to zero.\n",
        "temp = abs(gwbowv) < thres\n",
        "gwbowv[temp] = 0\n",
        "\n",
        "temp = abs(gwbowv_test) < thres\n",
        "gwbowv_test[temp] = 0\n",
        "\n",
        "#saving gwbowv train and test matrices\n",
        "np.save(\"/content/\"+gwbowv_name, gwbowv)\n",
        "np.save(\"/content/\"+test_gwbowv_name, gwbowv_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_PMB9EVwMeB"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "word2vec_model=model\n",
        "\n",
        "#document vector visualization\n",
        "## plain word2vec\n",
        "def plain_word2vec_document_vector(sentence,word2vec_model,num_features):\n",
        "    bag_of_centroids = np.zeros(num_features, dtype=\"float32\")\n",
        "\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            temp = word2vec_model[word]\n",
        "        except:\n",
        "            continue\n",
        "        bag_of_centroids += temp\n",
        "\n",
        "    bag_of_centroids =  bag_of_centroids / len(sentence)\n",
        "\n",
        "    return bag_of_centroids\n",
        "\n",
        "plainDocVec_all = {}\n",
        "counter = 0\n",
        "num_features = 200\n",
        "\n",
        "for review in all[\"news\"]:\n",
        "    # Get the wordlist in each news article.\n",
        "    result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
        "    result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
        "    h = result.split(\" \")\n",
        "    h = filter((\"\").__ne__, h)\n",
        "    words = list(h)\n",
        "    plainDocVec_all[counter] = plain_word2vec_document_vector(words,word2vec_model,num_features)\n",
        "    counter+=1\n",
        "    if counter % 1000 == 0:\n",
        "        print (\"All News Covered : \",counter)\n",
        "\n",
        "## visualize all document vector\n",
        "emb_tuple = tuple([plainDocVec_all[v] for v in plainDocVec_all.keys()])\n",
        "X = np.vstack(emb_tuple)\n",
        "\n",
        "plain_w2v_doc= TSNE(n_components=2, random_state=0,verbose=2)\n",
        "np.set_printoptions(suppress=True)\n",
        "plain_w2v_doc.fit(X)\n",
        "\n",
        "alldoc_plainw2v_tsne = pd.DataFrame(plain_w2v_doc.embedding_[:, 0],columns = [\"x\"])\n",
        "alldoc_plainw2v_tsne[\"y\"] = pd.DataFrame(plain_w2v_doc.embedding_[:, 1])\n",
        "alldoc_plainw2v_tsne[\"class\"] = list(all[\"class\"])\n",
        "\n",
        "sns.lmplot(data=alldoc_plainw2v_tsne,x=\"x\",y=\"y\",hue=\"class\",fit_reg=False,size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d1nIYQzwOPZ"
      },
      "outputs": [],
      "source": [
        "## test lgb\n",
        "from sklearn.metrics import classification_report\n",
        "import lightgbm as lgb\n",
        "\n",
        "start = time.time()\n",
        "clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
        "clf.fit(gwbowv, train[\"class\"])\n",
        "Y_true, Y_pred  = test[\"class\"], clf.predict(gwbowv_test)\n",
        "print (\"Report\")\n",
        "print (classification_report(Y_true, Y_pred, digits=6))\n",
        "print (\"Accuracy: \",clf.score(gwbowv_test,test[\"class\"]))\n",
        "print (\"Time taken:\", time.time() - start, \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SCDV_Test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQ0rQmg+l9rvWE4enYH+go",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}