{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JPMA TF1-1 2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1JPmLcjEFw-3y4eRmbaPyIHO3ZqUxJDUw",
      "authorship_tag": "ABX9TyNMYlfZLKyXq3rLPi5NTF7H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanakt-hub/Test/blob/main/Keyword_Match.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Txkzo3QhIfL5",
        "outputId": "de2d374a-c362-4ddf-91ec-e4909d0ce118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cholera due to Vibrio cholerae 01, biovar cholerae\n",
            "Cholera due to Vibrio cholerae 01, biovar eltor\n",
            "Cholera, unspecified\n",
            "Typhoid fever, unspecified\n",
            "Typhoid meningitis\n",
            "Typhoid fever with heart involvement\n",
            "review\n",
            "\"I've tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia & anxiety. My doctor suggested and changed me onto 45mg mirtazapine and this medicine has saved my life. Thankfully I have had no side effects especially the most common - weight gain, I've actually lost alot of weight. I still have suicidal thoughts but mirtazapine has saved me.\"\n",
            "\"My son has Crohn's disease and has done very well on the Asacol.  He has no complaints and shows no side effects.  He has taken as many as nine tablets per day at one time.  I've been very happy with the results, reducing his bouts of diarrhea drastically.\"\n",
            "\"Quick reduction of symptoms\"\n",
            "\"Contrave combines drugs that were used for alcohol, smoking, and opioid cessation. People lose weight on it because it also helps control over-eating. I have no doubt that most obesity is caused from sugar/carb addiction, which is just as powerful as any drug. I have been taking it for five days, and the good news is, it seems to go to work immediately. I feel hungry before I want food now. I really don't care to eat; it's just to fill my stomach. Since I have only been on it a few days, I don't know if I've lost weight (I don't have a scale), but my clothes do feel a little looser, so maybe a pound or two. I'm hoping that after a few months on this medication, I will develop healthier habits that I can continue without the aid of Contrave.\"\n",
            "\"I have been on this birth control for one cycle. After reading some of the reviews on this type and similar birth controls I was a bit apprehensive to start. Im giving this birth control a 9 out of 10 as I have not been on it long enough for a 10. So far I love this birth control! My side effects have been so minimal its like Im not even on birth control! I have experienced mild headaches here and there and some nausea but other than that ive been feeling great! I got my period on cue on the third day of the inactive pills and I had no idea it was coming because I had zero pms! My period was very light and I barely had any cramping! I had unprotected sex the first month and obviously didn't get pregnant so I'm very pleased! Highly recommend\"\n"
          ]
        }
      ],
      "source": [
        "#import files\n",
        "import csv\n",
        "import html\n",
        "\n",
        "raw_icd = \"/content/drive/MyDrive/Colab Notebooks/ICD10/icd10cm_codes_2022_disease.txt\"\n",
        "raw_rev = \"/content/drive/MyDrive/Colab Notebooks/ICD10/drugsComTest_raw.tsv\"\n",
        "\n",
        "ICD=[]\n",
        "REV=[]\n",
        "\n",
        "#ファイルを読み込み\n",
        "with open(raw_icd) as f:\n",
        "    reader = csv.reader(f, delimiter='\\t')\n",
        "    for row in reader:\n",
        "        ICD.append(str(row[0]))\n",
        "\n",
        "#ファイルを読み込み\n",
        "with open(raw_rev) as f:\n",
        "    reader = csv.reader(f, delimiter='\\t')\n",
        "    for row in reader:\n",
        "        REV.append(html.unescape(str(row[3])))\n",
        "\n",
        "for t, i in enumerate(ICD):\n",
        "  print(i)\n",
        "  if t == 5:\n",
        "    break\n",
        "\n",
        "for t, i in enumerate(REV):\n",
        "  print(i)\n",
        "  if t == 5:\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"book\")\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "pxpgp_1AJ7fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9356779-1f38-438f-801f-ed85da3a7d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "品詞：\n",
        "https://qiita.com/m__k/items/ffd3b7774f2fde1083fa"
      ],
      "metadata": {
        "id": "NgLFPIWn2Xq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "sentence = ''\n",
        "\n",
        "#ICDを1つにまとめる\n",
        "for i, s in enumerate(ICD):\n",
        "  sentence = sentence + ICD[i].lower().replace('-',' ').replace(\"'s\",'') + \". \" \n",
        "\n",
        "#Tokenaze\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "#4文字以上のトークンを抽出\n",
        "wordsFiltered = []\n",
        "for w in words:\n",
        "    if w not in stop_words:\n",
        "      if len(w) > 3:\n",
        "        wordsFiltered.append(w)\n",
        "\n",
        "#重複トークンを削除してソート\n",
        "tagged = nltk.pos_tag(list(set(wordsFiltered)))\n",
        "tagged.sort()\n"
      ],
      "metadata": {
        "id": "rgLxCXq-KDhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#数値と形容詞を除外\n",
        "COND=[]\n",
        "\n",
        "for i in tagged:\n",
        "    cls = i[1]\n",
        "    exp = [\"JJ\", \"CD\", \"IN\", \"RB\",\",\"]\n",
        "    if cls not in exp :\n",
        "      COND.append(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "IQAzRaLoKxG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print test\n",
        "for i, tmp in enumerate(COND):\n",
        "  print(tmp[0])\n",
        "  if i == 10:\n",
        "    break\n",
        "print(len(COND))"
      ],
      "metadata": {
        "id": "2zf3K4ybM8kJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a3c146-2830-4fec-a09e-949d6465d26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abdomen\n",
            "abdovity\n",
            "abducent\n",
            "abductor\n",
            "ablation\n",
            "abnormalities\n",
            "abnormality\n",
            "abortion\n",
            "abortus\n",
            "abrasion\n",
            "abscedens\n",
            "5142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *\n",
        "print(text1[1:50])"
      ],
      "metadata": {
        "id": "rV6big-4N9zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REVのTokenaze\n",
        "comms=[]\n",
        "for comm in REV:\n",
        "  comms.append(nltk.Text(nltk.word_tokenize(comm)))\n"
      ],
      "metadata": {
        "id": "umgPup_cNkNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aaa=[]\n",
        "for j, piyo in enumerate(comms):\n",
        "  print(\"*** comm: \" + str(j) + \"***\")\n",
        "  for i, tt in enumerate(COND):\n",
        "    aaa = piyo.concordance_list(tt[0]) \n",
        "    for asd in aaa:\n",
        "      print(str(i) + \": \" + str(tt) + \" \" + asd.line)\n",
        "  if j == 10:\n",
        "    break   "
      ],
      "metadata": {
        "id": "YfmD6JnkOmVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "とりあえずICDの単語→対象Textの検索は完成。\n",
        "ただし完全一致なのでTypoは検出不可。\n",
        "\n",
        "対象TextもNLP→同じように4文字以上の単語を抽出して近しさを総当たり検索させる？？・・・マシンパワーかかりそう。"
      ],
      "metadata": {
        "id": "QPaxJhpZ3IgC"
      }
    }
  ]
}