{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanakt-hub/Test/blob/main/JPMA_2022_TF_1_1_demo_(2)Grid_Search%E3%81%A8%E7%B5%90%E6%9E%9C%E3%81%AE%E7%A2%BA%E8%AA%8D%E3%81%BE%E3%81%A7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 前準備"
      ],
      "metadata": {
        "id": "x9IrHGswK7M6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Driveの接続"
      ],
      "metadata": {
        "id": "58PaUIBOK_EL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmtTGuaRaB8M",
        "outputId": "ec66f20e-dfe9-4e5a-99d3-2d8748603379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# データ受け渡しのためにGoogle Driveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# データ保存ディレクトリの指定\n",
        "datadir = '/content/drive/MyDrive/datadir/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データのロード"
      ],
      "metadata": {
        "id": "YCkmrjJ8kWMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 分かち書き済みのテキストとベクトル化したデータを読み込み\n",
        "import pickle\n",
        "\n",
        "with open(datadir + 'datadic.pkl', 'rb') as f:\n",
        "  datadic = pickle.load(f)"
      ],
      "metadata": {
        "id": "KzIB-qnnkObH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqjwea2bU7LN"
      },
      "source": [
        "# GridSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 検索対象モデルをインポート"
      ],
      "metadata": {
        "id": "YdMCHMetL9kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "\n",
        "# ロジスティック回帰とリッジ回帰\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "\n",
        "# 線形SVCとSVC\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "\n",
        "# ナイーブベイズ\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# k近傍法\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 決定木\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# ランダムフォレスト、ADAブースト、lightGBM\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "import lightgbm\n",
        "\n",
        "# ニューラルネットワーク\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "rF7I0TYaiiTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 検索パラメータと実行関数の設定"
      ],
      "metadata": {
        "id": "fzfX4GI5M6Mx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttnUevKXqXI-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Grid search対象のモデル\n",
        "clf_LR   = LogisticRegression(max_iter=50000)\n",
        "clf_RID  = RidgeClassifier(max_iter=50000)\n",
        "clf_LSVC = LinearSVC(max_iter=50000)\n",
        "clf_SVC  = SVC(probability=True)\n",
        "clf_NB   = BernoulliNB()\n",
        "clf_KN   = KNeighborsClassifier()\n",
        "clf_RF   = RandomForestClassifier()\n",
        "clf_AB   = AdaBoostClassifier()\n",
        "clf_LGB  = lightgbm.LGBMClassifier(objective='binary')\n",
        "clf_MLP  = MLPClassifier(max_iter=5000)\n",
        "\n",
        "# Grid searchのパラメータ範囲\n",
        "param_LR = {\n",
        "    'C': [10**i for i in range(-5,5)]\n",
        "     }\n",
        "\n",
        "param_RID = {\n",
        "    'alpha':  [10**i for i in range(-5,5)]\n",
        "     }\n",
        "\n",
        "param_LSVC = {\n",
        "    'C': [10**i for i in range(-5,5)]\n",
        "     }\n",
        "\n",
        "param_SVC = {\n",
        "    'C': [10**i for i in range(-2,5)], \n",
        "    'kernel': ['rbf'], \n",
        "    'gamma': [10**i for i in range(-5,2)]\n",
        "     }\n",
        "\n",
        "param_NB = {\n",
        "    'alpha':  [10**i for i in range(-5,5)]\n",
        "    }\n",
        "\n",
        "param_KN = {\n",
        "    'n_neighbors': np.arange(3, 15),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "\n",
        "param_RF = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'n_estimators': [100, 300, 500, 1000],\n",
        "    'max_depth': [7, 8, 9]\n",
        "}\n",
        "\n",
        "param_LGB = {\n",
        "      'num_leaves': [2, 4, 8, 16, 32, 64],\n",
        "      'reg_alpha': [0, 0.1, 1],\n",
        "      'reg_lambda': [0, 0.1, 1]\n",
        "}\n",
        "\n",
        "# ADAブーストとMLPは時間がかかるため実行用に検索パラメータを分割設定\n",
        "param_AB1 = {\n",
        "    'base_estimator':[DecisionTreeClassifier(max_depth=x) for x in range(7, 9)],\n",
        "    'n_estimators': [100, 300, 500, 1000],\n",
        "    'learning_rate' : [1.5]\n",
        "}\n",
        "\n",
        "param_AB2 = {\n",
        "    'base_estimator':[DecisionTreeClassifier(max_depth=x) for x in range(7, 9)],\n",
        "    'n_estimators': [100, 300, 500, 1000],\n",
        "    'learning_rate' : [1.0]\n",
        "}\n",
        "\n",
        "param_MLP1 = {\n",
        "    'hidden_layer_sizes': [(64), (128), (64,64)],\n",
        "    'learning_rate_init': [10**i for i in range(-4,-1)],\n",
        "    'alpha': [10**i for i in range(-6,-3)],\n",
        "    }\n",
        "param_MLP2 = {\n",
        "    'hidden_layer_sizes': [(128,128)],\n",
        "    'learning_rate_init': [10**i for i in range(-4,-1)],\n",
        "    'alpha': [10**i for i in range(-6,-3)],\n",
        "    }\n",
        "param_MLP3 = {\n",
        "    'hidden_layer_sizes': [(64,64,64)],\n",
        "    'learning_rate_init': [10**i for i in range(-4,-1)],\n",
        "    'alpha': [10**i for i in range(-6,-3)],\n",
        "    }\n",
        "\n",
        "\n",
        "# Grid Searchの実行関数定義\n",
        "def grid_search(clf, param, Data, Label):\n",
        "  grid_search = GridSearchCV(\n",
        "      clf,\n",
        "      param,\n",
        "      cv=5,         # 5-fold closs validation\n",
        "      scoring='f1') # f1でスコアリング\n",
        "  return grid_search.fit(Data, Label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 実行"
      ],
      "metadata": {
        "id": "DLdFw4jwN1xi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOj6AXtCEUnu"
      },
      "outputs": [],
      "source": [
        "###########################################################################\n",
        "### ！注意！                                                            ###\n",
        "### このまま実行すると完了までに20-30時間かかります。                   ###\n",
        "### 無料のColab環境ではタイムアウトしないよう実行対象を絞ってください。 ###\n",
        "### （Notebookを複製した並列実行も可能）                                ###\n",
        "###                                                                     ###\n",
        "### 参考実行時間：                                                      ###\n",
        "###  BoW:                                                               ###\n",
        "###   2.1時間（All）                                                    ###\n",
        "###  TFIDF:                                                             ###\n",
        "###   2.2時間（All）                                                    ###\n",
        "###  Emb:                                                               ###\n",
        "###   1.8時間（LR～LGB）｜8.5時間（AB1/2）｜1.0時間（MLP1-3）           ###\n",
        "###  Emb_tfidf:                                                         ###\n",
        "###   1.8時間（LR～LGB）｜7.6時間（AB1/2）｜1.8時間（MLP1-3）           ###\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "# 検索対象のモデルとパラメータ設定\n",
        "clfs = {\n",
        "    \"LR\"  : [clf_LR, param_LR],\n",
        "    \"RID\" : [clf_RID, param_RID],\n",
        "    \"LSVC\": [clf_LSVC, param_LSVC], \n",
        "    \"SVC\" : [clf_SVC, param_SVC], \n",
        "    \"NB\"  : [clf_NB, param_NB], \n",
        "    \"KN\"  : [clf_KN, param_KN], \n",
        "    \"RF\"  : [clf_RF, param_RF], \n",
        "    \"LGB\" : [clf_LGB, param_LGB],\n",
        "    \"AB1\" : [clf_AB, param_AB1], \n",
        "    \"AB2\" : [clf_AB, param_AB2], \n",
        "    \"MLP1\": [clf_MLP, param_MLP1],     \n",
        "    \"MLP2\": [clf_MLP, param_MLP2],     \n",
        "    \"MLP3\": [clf_MLP, param_MLP3],     \n",
        "  }\n",
        "\n",
        "# 実行対象ベクトルの設定\n",
        "vecs = [\n",
        "    'BoW',\n",
        "    'TFIDF',\n",
        "    'Emb',\n",
        "    'Emb_tfidf',\n",
        "]\n",
        "\n",
        "\n",
        "for vecname in vecs:\n",
        "  # 対象ベクトルを random seed = 0 で分割\n",
        "  X_train, X_test, y_train, y_test = train_test_split(datadic[vecname].astype(float), datadic['flg'], test_size=0.15, random_state = 0)\n",
        "  print('*** dataset:', vecname)\n",
        "\n",
        "  for clfname in clfs.keys():\n",
        "\n",
        "    # 分割した Train データで Grid Search CV を実施\n",
        "    print('  *** classifier:', clfname)\n",
        "    gs = grid_search(clfs[clfname][0], clfs[clfname][1], X_train, y_train)\n",
        "\n",
        "    # datadir に作成済みの結果 Dict があれば読み込み、なければ新規作成\n",
        "    try:\n",
        "      with open(datadir + vecname + '.pkl', 'rb') as f:\n",
        "        gsdic = pickle.load(f)\n",
        "    except:\n",
        "      gsdic = {}\n",
        "\n",
        "    # Grid Search CV の結果を保存\n",
        "    gsdic[clfname] = gs\n",
        "    gsdic[clfname + '_score'] = gs.score(X_test, y_test)\n",
        "\n",
        "    with open(datadir + vecname + '.pkl', 'wb') as f:\n",
        "      pickle.dump(gsdic, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix: 実行結果の確認"
      ],
      "metadata": {
        "id": "15v8DUgToxBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 全てのモデルの Grid Search 完了後に結果をまとめてCSV出力する\n",
        "# 結果閲覧のみを目的とするため、実行せずとも本編に影響なし\n",
        "\n",
        "# 本セクションのみの実行を想定して再マウント＆変数定義\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "datadir = '/content/drive/MyDrive/datadir/'\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "vecs = [\n",
        "    'BoW',\n",
        "    'TFIDF',\n",
        "    'Emb',\n",
        "    'Emb_tfidf',\n",
        "]\n",
        "\n",
        "# 各DictのKey確認\n",
        "for dataname in vecs: \n",
        "  with open(datadir + dataname + '.pkl', 'rb') as f:\n",
        "    gsmodels = pickle.load(f)\n",
        "  print(dataname)\n",
        "  print(gsmodels.keys())"
      ],
      "metadata": {
        "id": "z_-ozTeYUXV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a6e30b-d18d-4fd5-ada0-16f2550cd2b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "BoW\n",
            "dict_keys(['LR', 'LR_score', 'RID', 'RID_score', 'LSVC', 'LSVC_score', 'SVC', 'SVC_score', 'NB', 'NB_score', 'KN', 'KN_score', 'RF', 'RF_score', 'LGB', 'LGB_score', 'AB1', 'AB1_score', 'AB2', 'AB2_score', 'MLP1', 'MLP1_score', 'MLP2', 'MLP2_score', 'MLP3', 'MLP3_score'])\n",
            "TFIDF\n",
            "dict_keys(['LR', 'LR_score', 'RID', 'RID_score', 'LSVC', 'LSVC_score', 'SVC', 'SVC_score', 'NB', 'NB_score', 'KN', 'KN_score', 'RF', 'RF_score', 'LGB', 'LGB_score', 'AB1', 'AB1_score', 'AB2', 'AB2_score', 'MLP1', 'MLP1_score', 'MLP2', 'MLP2_score', 'MLP3', 'MLP3_score'])\n",
            "Emb\n",
            "dict_keys(['LR', 'LR_score', 'RID', 'RID_score', 'LSVC', 'LSVC_score', 'SVC', 'SVC_score', 'NB', 'NB_score', 'KN', 'KN_score', 'RF', 'RF_score', 'LGB', 'LGB_score', 'MLP1', 'MLP1_score', 'MLP2', 'MLP2_score', 'MLP3', 'MLP3_score', 'AB1', 'AB1_score', 'AB2', 'AB2_score'])\n",
            "Emb_tfidf\n",
            "dict_keys(['LR', 'LR_score', 'RID', 'RID_score', 'LSVC', 'LSVC_score', 'SVC', 'SVC_score', 'NB', 'NB_score', 'KN', 'KN_score', 'RF', 'RF_score', 'LGB', 'LGB_score', 'MLP3', 'MLP3_score', 'MLP1', 'MLP1_score', 'MLP2', 'MLP2_score', 'AB1', 'AB1_score', 'AB2', 'AB2_score'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JRJoo2qcRJSY"
      },
      "outputs": [],
      "source": [
        "# 保存された結果の読み込みとCSV出力の実行\n",
        "gsresults = pd.DataFrame()\n",
        "gsscores = []\n",
        "for dataname in vecs:\n",
        "  cv_results = pd.DataFrame()\n",
        "  with open(datadir + dataname + '.pkl', 'rb') as f:\n",
        "    gsmodels = pickle.load(f)\n",
        "\n",
        "  for clf in gsmodels.keys():\n",
        "    if clf.endswith('_score'):\n",
        "      clf_bestscores =  [dataname, str(clf), gsmodels[clf]]\n",
        "      gsscores.append(clf_bestscores)\n",
        "    else:\n",
        "      clf_cv_results =  pd.DataFrame(gsmodels[clf].cv_results_)\n",
        "      clf_cv_results.insert(0,\"clf\", str(clf))\n",
        "\n",
        "      cv_results = pd.concat([cv_results,clf_cv_results], axis = 0)\n",
        "\n",
        "  cv_results.insert(0,\"DataName\", dataname)\n",
        "\n",
        "  gsresults = pd.concat([gsresults,cv_results], axis = 0)\n",
        "\n",
        "gsscoresdf = pd.DataFrame(gsscores, columns=[\"DataName\",\"clf\",\"TestScore\"])\n",
        "gsresults.to_csv(datadir + 'gs_results.csv')\n",
        "gsscoresdf.to_csv(datadir + 'gs_scores.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}